# Together AI Configuration
TOGETHER_API_KEY=your_together_api_key_here
TOGETHER_EMBEDDING_MODEL=BAAI/bge-large-en-v1.5
TOGETHER_LLM_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo
TOGETHER_VISION_MODEL=meta-llama/Llama-Vision-Free

# Pinecone Configuration
PINECONE_API_KEY=your_pinecone_api_key_here
PINECONE_ENVIRONMENT=your_pinecone_environment_here
PINECONE_INDEX_NAME=hackrx-multimodal

# Authentication
BEARER_TOKEN=your_bearer_token_here

# LlamaParse Configuration (Optional)
LLAMA_PARSE_API_KEY=your_llama_parse_api_key_here

# Processing Settings
MAX_FILE_SIZE=52428800
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
MAX_CHUNKS_PER_QUERY=15
MAX_RESPONSE_TIME=30

# Performance Settings
EMBEDDING_BATCH_SIZE=50
MAX_CONCURRENT_EMBEDDINGS=10
MAX_CONCURRENT_LLM_CALLS=5

# Multimodal Settings
ENABLE_IMAGE_EXTRACTION=true
ENABLE_TABLE_EXTRACTION=true
ENABLE_VISION_ANALYSIS=true
IMAGE_QUALITY_THRESHOLD=0.7

# LlamaParse Settings
LLAMA_PARSE_RESULT_TYPE=markdown